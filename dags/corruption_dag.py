"""
Airflow DAG for running corruption experiments on the German Credit dataset.

This DAG:
- Loads the original raw dataset from PostgreSQL,
- Applies a corruption strategy (missing values, outliers, categorical errors, etc.),
- Saves the corrupted dataset,
- Runs DQ validation automatically via DQ_RESULTS_DATASET trigger,
- Produces metrics describing how corruption impacts data quality.

The output dataset triggers downstream DQ validation DAGs.
"""

from __future__ import annotations
import logging
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from dags.datasets import DQ_RESULTS_DATASET

logger = logging.getLogger("airflow.task")


def dq_corruption() -> list:
    """
    Execute a full corruption experiment workflow.

    Steps:
        1. Load original clean data from PostgreSQL.
        2. Apply corruption function selected in project config.
        3. Save the corrupted dataset with a timestamped dataset_id.
        4. Run DQ validation on the corrupted version.
        5. Return aggregated DQ statistics for each corruption scenario.

    Returns:
        list: Collection of DQ statistics generated by the experiment.
    """
    logger.info("=== STARTING DQ CORRUPTION ===")

    from scripts.get_environment_config import ProjectConfig
    from sql.postgres_manager import PostgresManager
    from degradation_strategies.corruption_experiment_runner import run_corruption_experiment

    cfg = ProjectConfig()

    # Load original data from raw table
    table_name = cfg.table_names["raw"]
    db_manager = PostgresManager()

    df = db_manager.load_clean_data(
        data_source="original_csv",
        table_name=table_name,
    )

    # Experiment configuration
    corruption_function = cfg.get_corruption_function()
    scenario = cfg.get_scenario()
    target_table_name = cfg.table_names["corrupted"]

    timestamp = datetime.now().strftime("%y%m%d_%H%M")
    dataset_id = f"corrupt_{timestamp}"

    logger.info(f"Running corruption experiment: {scenario} ({corruption_function})")
    logger.info(f"Corrupted table target: {target_table_name}")
    logger.info(f"Assigned dataset_id: {dataset_id}")

    results = run_corruption_experiment(
        original_df=df,
        scenario=scenario,
        corruption_function_name=corruption_function,
        dataset_id=dataset_id,
        target_table_name=target_table_name,
    )

    # Log and aggregate experiment results
    stat_corrs = []
    logger.info("=== CORRUPTION EXPERIMENT SUMMARY ===")

    for scenario_name, result in results.items():
        if "error" not in result:
            stats = result.get("statistics", {})
            success_pct = stats.get("success_percent", 0)
            stat_corrs.append(stats)
            logger.info(f"{scenario_name.upper()}: {success_pct:.1f}% DQ success rate")

    logger.info(f"Final collected DQ stats: {stat_corrs}")
    return stat_corrs


# ------------------------------ DAG DEFINITION ------------------------------ #

default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2025, 1, 12),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=2),
}

with DAG(
    "german_credit_dq_corruption",
    default_args=default_args,
    description="Data corruption experiment pipeline for German Credit dataset",
    schedule="@daily",
    catchup=False,
    tags=["data_quality"],
) as dag:

    run_dq_corruption = PythonOperator(
        task_id="dq_corruption",
        python_callable=dq_corruption,
        outlets=[DQ_RESULTS_DATASET],  # triggers downstream DQ validation DAG
    )

    run_dq_corruption
